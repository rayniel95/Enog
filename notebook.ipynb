{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Intoduccion\n",
    "\n",
    "El siguiente proyecto tiene por objetivo el uso de ***redes neuronales*** con el fin de realizar clasificacion de texto y reconocer entidades cubanas. Con entidades cubanas nos referimos a cualquier ente que pertenezca a Cuba, pueden ser desde animales autoctonos, instituciones, personas, marcas de empresas, productos, platos gastronomicos, etc. cualquier cosa que lleve el sello de cubania, la idea es decir si es una entidad cubana y de que tipo. Las entidades cubanas se encuentran en el texto en forma de frases, por ejemplo: Jose Marti, Republica de Cuba, etc. Entre los tipos de entidades cubanas se encuentran *ley*, *organizacion*, *persona*, etc., el tipo *otro* es utilizado para clasificar cualquier palabra no relacionada con una entidad cubana. Como se puede apreciar tal tarea es casi imposible de cometer, principalmente por el tamano del espacio de entidades cubanas y los distintos tipos de las mismas, clasificar todas las entidades cubanas en texto es una tarea titanica, no digamos ya crear un buen clasificador. Sin embargo, en este proyecto intentamos, con un dataset bastante modesto, realizar ese objetivo. Se probaron varios modelos, los cuales veremos desde el mas simple hasta el mas complejo que es el estado del arte."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Baseline\n",
    "\n",
    "El primer modelo es un *baseline*, la idea es crear un clasificador que dado el trainset, le asignara a cada palabra la categoria en la que esta aparezca mas, posteriormente para clasificar el testset, si la palabra se encuentra almacenada y clasificada con la categoria en la que mas se repite en el trainset, se devuelve esta categoria, si no se devuelve la categoria de palabra que no pertenece a entidades cubanas, es decir *otro*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.base\n",
    "from sklearn.metrics import f1_score, make_scorer, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "source": [
    "Simple metodo que lee el dataset y lo devuelve."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(name: str):\n",
    "\tdata = None\n",
    "\twith open(name, 'r', encoding='utf-8') as file:\n",
    "\t\tdata = file.read()\n",
    "\treturn data"
   ]
  },
  {
   "source": [
    "Metodo que transforma de la representacion del dataset a una representacion **BIO**, la representacion **BIO** debe su nombre a *begin*, *in*, *other*, la idea es poder conocer que frases de un texto son consideradas entidades cubanas, en este caso la primera palabra de la frase sera *taggeada* (del spanglish *taggear* que se deriba del ingles *tag* y tiene por significado categorizar, clasificar, marcar, etc.) de *begin* y las restantes palabras que se encuentren en la frase seran *taggueadas* de *in*. De esta forma, por ejemplo, las palabras de una organizacion como pudiera ser Union de Jovenes Comunistas, terminaria con las palabras categorizadas del siguiente modo: `Union:begin-ORG`, `de:in-ORG`, `Jovenes:in-ORG`, `Comunistas:in-ORG`. En donde *ORG* se refiere a organizacion, de esta forma podemos conocer que esta entidad es cubana y es una organizacion. Esta forma de clasificar tiene la ventaja de permitirnos conocer donde inicia, termina y las palabras internas de una entidad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tsv2BIO(name: str) -> Tuple[List[str], List[str]]:\n",
    "\twords = []\n",
    "\ttags = []\n",
    "\tactual: str = ''\n",
    "\twith open(name, 'r', encoding='utf-8') as file:\n",
    "\t\tfor item in file.readlines():\n",
    "\t\t\tword: str\n",
    "\t\t\ttag: str\n",
    "\t\t\tword, tag = item.replace('\\n', '').split('\\t')\n",
    "\t\t\tif tag != 'O':\n",
    "\t\t\t\twords_in = word.split()\n",
    "\t\t\t\twords.append(words_in.pop(0))\n",
    "\t\t\t\ttags.append(f'B-{tag}')\n",
    "\t\t\t\tfor word_in in words_in:\n",
    "\t\t\t\t\twords.append(word_in)\n",
    "\t\t\t\t\ttags.append(f'I-{tag}')\n",
    "\t\t\telse:\n",
    "\t\t\t\twords.append(word)\n",
    "\t\t\t\ttags.append(tag)\n",
    "\n",
    "\treturn words, tags"
   ]
  },
  {
   "source": [
    "Elimina las categorias que no pertenezcan a organizacion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_not_org_tags(tags: List[str]) -> List[str]:\n",
    "\tn_tags = []\n",
    "\ttag: str\n",
    "\tfor tag in tags:\n",
    "\t\tif tag != 'O' and tag.split('-')[1] != 'ORG':\n",
    "\t\t\tn_tags.append('O')\n",
    "\t\telse: n_tags.append(tag)\n",
    "\treturn n_tags"
   ]
  },
  {
   "source": [
    "Transforma las categorias en numeros, categorias numericas, de esta forma se podra entrenar las *redes neuronales* mas adelante."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categories(a_list: List[str]) -> List[int]:\n",
    "\tcats = {}\n",
    "\tindex = 0\n",
    "\tother_list = []\n",
    "\tfor tag in a_list:\n",
    "\t\ttry:\n",
    "\t\t\tother_list.append(cats[tag])\n",
    "\t\texcept:\n",
    "\t\t\tcats[tag] = index\n",
    "\t\t\tindex += 1\n",
    "\t\t\tother_list.append(cats[tag])\n",
    "\n",
    "\treturn other_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature_array(feature_names: List[str], feature_vectors: List[Dict[str, Any]]):\n",
    "\n",
    "\tnew_features = []\n",
    "\tfor vector in feature_vectors:\n",
    "\t\tnew_features.append([vector.get(name, 0) for name in feature_names])\n",
    "\n",
    "\treturn new_features"
   ]
  },
  {
   "source": [
    "Dado un texto como una sola cadena lo divide en oraciones por los puntos. Tambien utilizado para dividir el texto por cantidad de oraciones, util para transformar la cadena del texto en trainset y testset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter:\n",
    "\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tif kwargs.get('words', False):\n",
    "\t\t\tself.text = self.to_sentences(kwargs['words'])\n",
    "\t\telse: raise Exception('incorrect arguments')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef to_sentences(sents: List[str]) -> List[List[str]]:\n",
    "\t\tresp = []\n",
    "\t\tsents_cop = sents\n",
    "\t\tfor times in range(sents.count('.')):\n",
    "\t\t\tpart = sents_cop[: sents_cop.index('.') + 1]\n",
    "\t\t\tresp.append(part)\n",
    "\t\t\tsents_cop = sents_cop[sents_cop.index('.') + 1:]\n",
    "\t\treturn resp\n",
    "\n",
    "\tdef split(self, number: int) -> Tuple[List[str], List[str]]:\n",
    "\t\tpart1, part2 = self.text[: number], self.text[number:]\n",
    "\t\tt1 = []\n",
    "\t\tret1 = [t1.extend(ls) for ls in part1][0]\n",
    "\t\tt2 = []\n",
    "\t\tret2 = [t2.extend(ls) for ls in part2][0]\n",
    "\n",
    "\t\treturn t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tags(vectors, tags_index: dict):\n",
    "\tvektor = list(vectors)\n",
    "\ttags = []\n",
    "\tindex2tags = {value: key for key, value in tags_index.items()}\n",
    "\tfor sentence in vektor:\n",
    "\t\tsent = []\n",
    "\t\tfor vec in sentence:\n",
    "\t\t\tsent.append(index2tags[list(vec).index(1.0)])\n",
    "\t\ttags.append(sent)\n",
    "\n",
    "\tnew_tags = []\n",
    "\n",
    "\tfor el in tags:\n",
    "\t\tnew_tags.extend(el)\n",
    "\n",
    "\treturn new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model_maker, train: np.ndarray, target: list, cv=3, epochs=30,\n",
    "\t\t\t  batch_size=32, validation_split=0.1):\n",
    "\tmodel: keras.Model\n",
    "\tnum_validation_samples = len(train) // cv\n",
    "\tvalidation_scores = []\n",
    "\n",
    "\tfor fold in range(cv):\n",
    "\t\tvalidation_data = train[num_validation_samples * fold:\n",
    "\t\t\t\t\t\t\t   num_validation_samples * (fold + 1)]\n",
    "\t\ttarget_validation_data = target[num_validation_samples * fold:\n",
    "\t\t\t\t\t\t\t   num_validation_samples * (fold + 1)]\n",
    "\n",
    "\t\ttraining_data = train[:num_validation_samples * cv].tolist() + \\\n",
    "\t\t\ttrain[num_validation_samples * (cv + 1):].tolist()\n",
    "\t\ttraining_data = np.array(training_data)\n",
    "\n",
    "\t\ttarget_training_data = target[:num_validation_samples * cv] + \\\n",
    "\t\t\ttarget[num_validation_samples * (cv + 1):]\n",
    "\n",
    "\t\tmodel = model_maker()\n",
    "\t\tmodel.fit(training_data, np.array(target_training_data), epochs=epochs,\n",
    "\t\t\t\t  batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "\t\tvalidation_score = model.evaluate(validation_data,\n",
    "\t\t\t\t\t\t\t\t\t\t  np.array(target_validation_data))\n",
    "\t\tvalidation_scores.append(validation_score)\n",
    "\n",
    "\treturn sum([el[0] for el in validation_scores]) / len(validation_scores), \\\n",
    "\t\tsum([el[1] for el in validation_scores]) / len(validation_scores)"
   ]
  },
  {
   "source": [
    "Clase encargada de como se hablo al inicio de la seccion de clasificar las palabras. El *baseline* en si mismo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTagger(sklearn.base.BaseEstimator):\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\t'''\n",
    "\t\tExpects a list of words as X and a list of tags as y.\n",
    "\t\t'''\n",
    "\t\tvoc = {}\n",
    "\t\tself.tags = []\n",
    "\t\tfor x, t in zip(X, y):\n",
    "\t\t\tif t not in self.tags:\n",
    "\t\t\t\tself.tags.append(t)\n",
    "\t\t\tif x in voc:\n",
    "\t\t\t\tif t in voc[x]:\n",
    "\t\t\t\t\tvoc[x][t] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvoc[x][t] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvoc[x] = {t: 1}\n",
    "\t\tself.memory = {}\n",
    "\t\tfor k, d in voc.items():\n",
    "\t\t\tself.memory[k] = max(d, key=d.get)\n",
    "\n",
    "\tdef predict(self, X, y=None):\n",
    "\t\t'''\n",
    "\t\tPredict the the tag from memory. If word is unknown, predict 'O'.\n",
    "\t\t'''\n",
    "\t\treturn [self.memory.get(x, 'O') for x in X]"
   ]
  },
  {
   "source": [
    "Se carga el *corpus* (texto clasificado) y lo dividimos en trainset y testset, para el testset utilizaremos las palabras de las ultimas 20 oraciones."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = transform_tsv2BIO('code/corpus.tsv')\n",
    "\n",
    "all_words = list(set(words))\n",
    "all_tags = list(set(tags))\n",
    "\n",
    "n_words = len(all_words)\n",
    "n_tags = len(all_tags)\n",
    "\n",
    "sent_get = SentenceGetter(words=words)\n",
    "w_train, w_test = sent_get.split(-20)\n",
    "t_train, t_test = tags[:len(w_train)], tags[len(w_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9105711385651969\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/classification.py:1436: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  precision = _prf_divide(tp_sum, pred_sum,\n"
     ]
    }
   ],
   "source": [
    "model = MemoryTagger()\n",
    "model.fit(w_train, t_train)\n",
    "\n",
    "pred = model.predict(w_test)\n",
    "\n",
    "print(precision_score(t_test, pred, average='weighted'))"
   ]
  },
  {
   "source": [
    "# Conditional Random Fields\n",
    "\n",
    "[Conditional Random Fields][1] fue una tecnica muy utilizada en procesamiento de lenguaje natural y clasificaciond de textos antes de que se extendiera el uso de las redes neuronales, aun puede verse esta tecnica comparandose con el estado del arte. Para aplicarla al problema que tenemos creamos una representacion vectorial de las palabras utilizando caracteristicas de la palabra en si misma.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Conditional_random_field"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent: sentence, i: int) -> Dict:\n",
    "\t# todo modificar los features para obtener mayor presicion con un contexto\n",
    "\t#  mas grande\n",
    "\tword = sent[i]\n",
    "\n",
    "\tfeatures = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "\tif i > 0:\n",
    "\t\tword1 = sent[i - 1]\n",
    "\t\tfeatures.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent) - 1:\n",
    "\t\tword1 = sent[i + 1]\n",
    "\t\tfeatures.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "source": [
    "Transformamos en dataset en estos *feature vectors* o vectores de caracteristicas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent: sentence):\n",
    "\treturn [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-208d1a49a7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                    \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_possible_transitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_train_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_train_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "w_train_f = sent2features(w_train)\n",
    "w_test_f = sent2features(w_test)\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1,\n",
    "\t\t\t\t\t\t   max_iterations=100, all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit([].append(w_train_f), [].append(t_train))\n",
    "\n",
    "# params_space = {'c1': scipy.stats.expon(scale=0.5),\n",
    "# \t\t\t\t'c2': scipy.stats.expon(scale=0.05),\n",
    "# }\n",
    "# print(len(w_train_f), len(w_train), len(t_train))\n",
    "# f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# rs = RandomizedSearchCV(crf, params_space, verbose=1, cv=3,\n",
    "# \t\t\t\t\t\tn_jobs=4, n_iter=200, scoring=f1_scorer)\n",
    "# rs.fit(w_train_f, t_train)\n",
    "\n",
    "# print('best params:', rs.best_params_)\n",
    "# print('best CV score:', rs.best_score_)\n",
    "# print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "# crf = rs.best_estimator_\n",
    "\n",
    "# t_pred2 = crf.predict(sent2features(['el', 'Ministerio', 'del', 'Interior',\n",
    "# \t\t\t\t\t\t\t\t\t  'y', 'la', 'Union', 'de', 'Jovenes',\n",
    "# \t\t\t\t\t\t\t\t\t  'Capitalistas', 'de', 'Chile',\n",
    "# \t\t\t\t\t\t\t\t\t  'participan', 'en', 'la', 'recogida',\n",
    "# \t\t\t\t\t\t\t\t\t  'de', 'materias', 'primas', '.']))\n",
    "# print(t_pred2)\n",
    "# new = to_feature_array(features, w_train_f)\n",
    "\n",
    "# learning_curve(crf, new, t_train, cv=3)\n",
    "\n",
    "# pred = crf.predict(w_test_f)\n",
    "\n",
    "# print(f1_score(t_test, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}